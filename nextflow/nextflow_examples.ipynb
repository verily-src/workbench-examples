{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Nextflow on Verily Workbench\n",
    "<table align=\"left\">\n",
    "<td>\n",
    "<a href=\"https://github.com/verily-src/workbench-examples/blob/main/nextflow/nextflow_examples.ipynb\">\n",
    "<img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "    View on GitHub\n",
    "</a>\n",
    "</td>\n",
    "<td>\n",
    "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/verily-src/workbench-examples/blob/main/nextflow/nextflow_examples.ipynb\">\n",
    "<img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "Open in a Verily Workbench notebook instance\n",
    "</a>\n",
    "</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### What is Nextflow?\n",
    "Nextflow is an open-source workflow orchestrator that simplifies writing and deploying data-intensive computational pipelines on any infrastructure.  \n",
    "\n",
    "More information and additional examples can be found in the [Nextflow documentation](https://nextflow.io/docs/latest/index.html).\n",
    "\n",
    "### Objectives\n",
    "\n",
    "This notebook is intended to demonstrate how you can use the Nextflow engine on Verily Workbench to execute and manage workflows. By running the cells in this notebook, you will be able to:\n",
    "* Configure Nextflow to run on this cloud environment\n",
    "* Run Nextflow workflows on actual datasets\n",
    "* Check the status of submitted jobs\n",
    "* Leverage the Google Cloud Life Sciences API to execute workflow tasks in parallel\n",
    "* View the logs for each step of the workflows executed\n",
    "* Clean up intermediate results to reduce cloud storage use\n",
    "\n",
    "Note that all of the commands demonstrated with the prefix `!` are shell commands, which you can also run from a command-line such as a Jupyter terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook setup <a id=\"workspace_setup\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Notebook dependencies\n",
    "\n",
    "Nextflow is installed by default in Verily Workbench workspaces, including this one. To test that it has been installed, you can run `nextflow -v`, which will output the installed version.\n",
    "\n",
    "For the provided examples to run successfully, you first need to run the cells in this section, which install the 'nf-core' tool, restart the kernel, and initialize variables that are utilized in one or both of the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install `nf-core` tool\n",
    "\n",
    "In order to interact with [`nf-core`](https://nf-co.re) pipelines in Example 2,  you'll need to install the [`nf-core` companion tool](https://nf-co.re/tools), a command-line interface.\n",
    "\n",
    "1. Run the first cell below to install the tool in your cloud environment. \n",
    "2. Once installation has completed, navigate to the **Kernel** dropdown in the JupyterLab menu and select **Restart Kernel**. \n",
    "3. Once the kernel has restarted, run the second cell below to import the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import nf_core\n",
    "    print(\"nf-core is already installed\")\n",
    "except:\n",
    "    print(\"Installing nf-core...\")\n",
    "    !pip install nf-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import nf_core\n",
    "    print(\"nf-core is already installed\")\n",
    "except:\n",
    "    print(\"Please restart the kernel before importing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Environment variables\n",
    "\n",
    "One advantage of running your workflows in Verily Workbench is the ability to inject parameters into your commands from the workspace context. You can pass parameter values such as your Google Cloud Storage bucket's URL or your service account's email address with convenient, readable names in CLI commands, preventing you the tedious and error-prone copying and pasting of values otherwise required. (To view this workspace's environment variables and their corresponding values, you can run the command `wb app execute env | sort`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check environment variables\n",
    "\n",
    "Upon running this notebook in a fresh cloud environment, you may discover that your environment variables are not available on your workspace context. Please run the cell below to check these variables. If the exception is raised, you should restart this cloud environment in the Verily Workbench UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def check_environment_variables():\n",
    "    '''\n",
    "    Check that this notebook VM's environment variables resolve to the \n",
    "    expected workspace context values.\n",
    "    '''\n",
    "    if not (os.getenv('GOOGLE_CLOUD_PROJECT')\n",
    "            and os.getenv('GOOGLE_SERVICE_ACCOUNT_EMAIL')\n",
    "            and os.getenv('WORKBENCH_USER_EMAIL')) :\n",
    "        raise Exception('Restart your Workbench Cloud Environment so that the updated environment variables become available.')       \n",
    "\n",
    "check_environment_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Run the cell below to capture the <a href=\"https://cloud.google.com/life-sciences/docs/concepts/locations\">geographic location of the physical resources</a> where your cloud environment exists. You will provide this value when configuring jobs to run via the Google Life Sciences API in the exercises that follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "REGION=subprocess.run(\"curl -s 'http://metadata.google.internal/computeMetadata/v1/instance/zone' -H 'Metadata-Flavor: Google' \\\n",
    "  | sed -e 's#^.*/##' -e 's#-[a-z]$##'\", stdout=subprocess.PIPE, shell=True)\n",
    "REGION=str(REGION.stdout, 'utf-8')\n",
    "print(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Workspace setup notebook\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook assumes that you have run <a href=\"../workspace_setup.ipynb\">workspace_setup.ipynb</a> in the parent directory to create necessary workspace resources. Please note that you can skip creating a BigQuery dataset as that won't be used in this notebook's examples.\n",
    "</div>\n",
    "     Running the `workspace_setup.ipynb` notebook will create two Cloud Storage buckets for your workspace files with workspace reference names: \n",
    "\n",
    " - ws_files   \n",
    " - ws_files_autodelete_after_two_weeks      \n",
    "    \n",
    "The code in this notebook will write output files to the \"autodelete\" bucket by default. Any file in this bucket will be automatically deleted two weeks after it is written. This alleviates the need for you to remember to clean up temporary and example files manually. If you want to write outputs to a durable location, simply change the assignment of the `BUCKET_REFERENCE` variable in the cell below and re-run the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change this to \"ws_files\" to use the durable workspace bucket instead of the autodelete bucket.\n",
    "BUCKET_REFERENCE = \"ws_files_autodelete_after_two_weeks\"\n",
    "\n",
    "!wb resource resolve --name $BUCKET_REFERENCE || echo \"Be sure to run workspace_setup.ipynb before this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example 1: Run a Nextflow workflow\n",
    "\n",
    "In this exercise, you'll run a Nextflow workflow on sample human data. Running this example will typically incur less than $1 in cloud costs.\n",
    "\n",
    "If desired, you can preview a fully-executed version of this notebook section by viewing [this notebook snapshot](TODO). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clone Nextflow RNASeq project\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> \n",
    "To access private GitHub repositories from your Verily Workbench workspace, you'll need to <a href=\"https://support.workbench.verily.com/docs/guides/cloud_environments/git_integrations/#set-up-an-ssh-key\">set up an SSH key</a> to connect your GitHub and Verily Workbench accounts. This is not necessary for the exercises in this notebook because the repositories you will need are publicly available.\n",
    "</div>\n",
    "\n",
    "Verily Workbench features built-in support for source control via [GitHub](https://github.com). Users who have set up the Verily Workbench SSH key can run `terra git` commands from their Verily Workbench workspaces without additional authentication.\n",
    "\n",
    "In this exercise, you will use files from a public [GitHub repository](https://github.com/nextflow-io/rnaseq-nf.git) which contains a basic pipeline for quantification of genomic features from short read data and some data upon which to run the workflow.\n",
    "\n",
    "Run the cell below to check whether the GitHub repo exists as a referenced resource in this workspace. **If your workspace isn't a clone of the [Getting Started with Workflows workspace](https://TODO),** the command below will add the referenced resource and clone it to your home directory. If you see an error message like:\n",
    "```\n",
    "fatal: destination path 'rnaseq-nf' already exists and is not an empty directory.\n",
    "Git clone for https://github.com/nextflow-io/rnaseq-nf.git failed\n",
    "```\n",
    "that simply means your resource exists as we expect. *Do not* create another resource with a different name; just proceed to the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wb resource resolve --name=rnaseq-nf || (wb resource add-ref git-repo \\\n",
    "    --name=rnaseq-nf \\\n",
    "    --description=\"Respository containing a Nextflow RNASeq pipeline and associated input data.\" \\\n",
    "    --repo-url='https://github.com/nextflow-io/rnaseq-nf.git')\n",
    "!cd /home/jupyter && wb git clone --resource=rnaseq-nf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `Nextflow` configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Modify Nextflow configuration file\n",
    "\n",
    "Next, run the cell below to inject the necessary parameters into the Nextflow configuration file and output its contents.  \n",
    "In the `gls` entry, note the parameters set to workspace context variables: \n",
    "* `$WORKBENCH_{BUCKET_REFERENCE}`,\n",
    "* `$WORKBENCH_{REGION}`,\n",
    "* `$GOOGLE_CLOUD_PROJECT` and \n",
    "* `$GOOGLE_SERVICE_ACCOUNT_EMAIL`. \n",
    "\n",
    "If you've created the workspace bucket resource after you created this notebook instance (via either the [Verily Workbench workspace UI](https://support.workbench.verily.com/docs/guides/research_data/data_resources_operations/#create-a-storage-bucket) or [Workbench CLI commands](https://support.workbench.verily.com/docs/references/cli_reference/wb/resource/create/), your bucket reference may not resolve correctly due to a stale workspace context cache. To resolve the variable, you'll need to run `wb resource list` to refresh the cached workspace context, then re-run the cell below.\n",
    "\n",
    "Please note that `google.region` specifies the [Google location(s)](https://cloud.google.com/life-sciences/docs/concepts/locations) where the job executions are deployed to the Cloud Life Sciences API, whereas `google.location` specifies the [Google region(s)](https://cloud.google.com/compute/docs/regions-zones/) where the computation is executed on Compute Engine VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def configure_nextflow_for_wb():\n",
    "    \"\"\"\n",
    "    Injects workspace context variables as parameters & configures Nextflow to use \n",
    "    Google's Cloud Life Sciences API to run the RNASeq pipeline.\n",
    "    Provides Google Cloud URL for transcriptome and read files.\n",
    "    \"\"\"\n",
    "    wb_gls_config = f\"\"\"gls {{\n",
    "        params.transcriptome = 'gs://rnaseq-nf/data/ggal/transcript.fa'\n",
    "        params.reads = 'gs://rnaseq-nf/data/ggal/gut_{{1,2}}.fq'\n",
    "        params.multiqc = 'gs://rnaseq-nf/multiqc'\n",
    "        process.executor = 'google-lifesciences'\n",
    "        process.container = 'nextflow/rnaseq-nf:latest'\n",
    "        workDir = \"${{WORKBENCH_{BUCKET_REFERENCE}}}/nf\"\n",
    "        google.region  = \"{REGION}\"\n",
    "        google.project = \"$GOOGLE_CLOUD_PROJECT\"\n",
    "        google.lifeSciences.network = 'network'\n",
    "        google.lifeSciences.subnetwork = 'subnetwork'\n",
    "        google.lifeSciences.serviceAccountEmail = \"$GOOGLE_SERVICE_ACCOUNT_EMAIL\"\n",
    "        }}\"\"\"\n",
    "    \n",
    "    # Replace boilerplate with V-specific config for Google Lifesciences APIs.\n",
    "    regex = \"(?s)gls(\\s\\{.*?\\s\\})(?=\\s|$)\"\n",
    "    config_file = open(\"/home/jupyter/rnaseq-nf/nextflow.config\", \"r\")\n",
    "    data = config_file.read()\n",
    "    config_file.close()\n",
    "    result = re.sub(regex, wb_gls_config, data, 1)\n",
    "\n",
    "    if result:\n",
    "        config_file = open(\"/home/jupyter/rnaseq-nf/nextflow.config\", \"w\")\n",
    "        config_file.write(result)\n",
    "        config_file.close()\n",
    "\n",
    "# Copy existing configuration file before we modify it.\n",
    "!cp /home/jupyter/rnaseq-nf/nextflow.config /home/jupyter/rnaseq-nf/unmodified_nextflow.config\n",
    "\n",
    "# Inject configuration.\n",
    "configure_nextflow_for_wb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inspect your Nextflow config\n",
    "\n",
    "Run the cell below to validate your configuration for Nextflow in this workspace.  \n",
    "In particular, inspect the output to ensure the parameters highlighted (your workspace project, GCS bucket, and service account email) have been resolved to the appropriate values.  \n",
    "You should see something like below, with the appropriate values substituted for the placeholders in curly braces:\n",
    "<code>\n",
    "Setting the gcloud project to the workspace project\n",
    "Updated property [core/project].\n",
    "manifest {\n",
    "   description = 'Proof of concept of a RNA-seq pipeline implemented with Nextflow'\n",
    "   author = 'Paolo Di Tommaso'\n",
    "   nextflowVersion = '>=20.07.0'\n",
    "}\n",
    "</code><code>\n",
    "params {\n",
    "   transcriptome = 'gs://rnaseq-nf/data/ggal/transcript.fa'\n",
    "   reads = 'gs://rnaseq-nf/data/ggal/gut_{1,2}.fq'\n",
    "   multiqc = 'gs://rnaseq-nf/multiqc'\n",
    "}\n",
    "</code><code>\n",
    "process {\n",
    "   executor = 'google-lifesciences'\n",
    "   container = 'nextflow/rnaseq-nf:latest'\n",
    "}\n",
    "   workDir =</code><code style=\"background:yellow;color:black\">'{MY_BUCKET}/nf'</code>\n",
    "<code>\n",
    "google {\n",
    "   location = 'us-central1'\n",
    "   region = 'us-central1'\n",
    "   project = <code style=\"background:yellow;color:black\">'{MY_PROJECT}'</code>\n",
    "<code>\n",
    "    lifeSciences {\n",
    "        network = 'network'\n",
    "        subnetwork = 'subnetwork'\n",
    "        serviceAccountEmail = <code style=\"background:yellow;color:black\">'{MY_PET_SA}'</code>\n",
    "   }\n",
    "    }</code>\n",
    "Restoring the original gcloud project configuration: {MY_PROJECT}\n",
    "Updated property [core/project].\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wb nextflow -c /home/jupyter/rnaseq-nf/nextflow.config config /home/jupyter/rnaseq-nf/main.nf -profile gls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Launch RNASeq workflow\n",
    "\n",
    "Run the cell below to use the [Workbench CLI](https://support.workbench.verily.com/docs/guides/cli/cli_intro/) to launch a sample Nextflow workflow for [an RNA sequencing pipeline](https://github.com/nf-core/rnaseq),   \n",
    "which maps a collection of read-pairs to a given reference genome and outputs the respective transcript model.  \n",
    "The workflow should take about 10 minutes to complete. Once your job has completed, you should see output like:\n",
    "```\n",
    "Done! Open the following report in your browser --> results/multiqc_report.html\n",
    "\n",
    "Completed at: DD-Month-YYYY HH:MM:SS\n",
    "Duration    : 10m 41s\n",
    "CPU hours   : 0.2\n",
    "Succeeded   : 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wb nextflow -c /home/jupyter/rnaseq-nf/nextflow.config run /home/jupyter/rnaseq-nf/main.nf -profile gls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### View pipeline runs\n",
    "\n",
    "Run the cell below to print a history of the Nextflow pipelines you've executed. You should see something like:\n",
    "```\n",
    "TIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                                 COMMAND      \n",
    "YYYY-MM-DD 15:27:03\t10m 13s   \t<RUN_NAME>      \tOK    \t386f5387c7 \tfb368e91-ceab-4f8c-b17f-a575836e6d80\tnextflow -c rnaseq-nf/nextflow.config run rnaseq-nf/main.nf -profile gls   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### View execution details for a specific run\n",
    "\n",
    "To view the tasks executed by your pipeline during a specific run, replace the `RUN_NAME` parameter below with the run name corresponding to the nf_core RNASeq job from the output of the previous `nextflow log ...` command. Then run the cell below to see the tasks and their statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow log <RUN_NAME> -f 'task_id,name,status,duration,cpus,container' | sort -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### View workflow results\n",
    "\n",
    "The result of a successful run of the example workflow includes an HTML report from [MultiQC](https://multiqc.info/).\n",
    "To view the results, you can navigate in the JupyterLab file browser to the `multiqc_report.html` file in the `results` directory.\n",
    "The first time you open the report, it may include a message about JavaScript being disabled.\n",
    "To resolve this, select the \"Trust HTML\" button as described in [this JupyterLab issue](https://github.com/jupyterlab/jupyterlab/issues/9738).\n",
    "Alternatively, you may right click and open the MultiQC report in a new browser tab, which will not require clicking a button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Optional: Clean up intermediate results\n",
    "\n",
    "If you won't need to reference the intermediate results produced by each workflow step in the future and don't intend to run a pipeline again with the `--resume` flag, it's a good practice to remove the associated working directories & their contents from your workspace bucket. This reduces your storage cost and makes it easier to find results within your bucket when you do wish to inspect intermediate results from a particular run (i.e. for debugging or to analyze a particular step in the pipeline).   \n",
    "\n",
    "Once you've performed cleanup for a specific run, the associated cached intermediate results have been deleted and you can no longer resume pipeline execution from those results. Therefore, it's important to perform a dry-run of the cleanup first to view the directories that will be removed and ensure you don't inadvertently clean up any checkpointing you wish to keep. \n",
    "\n",
    "Run the cell below to perform a dry-run for a pipeline run by replacing `<RUN_NAME>` with the run name of your specific pipeline execution. If you'd like to cleanup for all previous pipeline runs, replace `<RUN_NAME>` with `$(nextflow log -q)` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow clean -n <RUN_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "If you're satisfied with deleting the pipeline working directories indicated above, replace `<RUN_NAME>` with the name of the desired pipeline execution in the cell below, then run the cell to permanently remove files associated with that run. \n",
    "\n",
    "If you'd prefer to clean up directories associated with all previous runs, you can replace `<RUN_NAME>` with `$(nextflow log -q)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow clean -f <RUN_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example 2: Run an `nf-core` workflow\n",
    "\n",
    "Nextflow's `nf-core` is a curated collection of validated Nextflow pipelines, most of which can be run on almost any computing environment, including your Verily Workbench cloud environment. These pipelines are subject to transparent versioning and each version is run against automated testing prior to release. In this exercise, we will perform RNASeq on by running the nf-core/RNASeq pipeline from the [`nf_core` collection](https://nf-co.re/pipelines). The cost of running this example will typically be no more than $1.\n",
    "\n",
    "If desired, you can preview a fully-executed version of this notebook section by viewing [this notebook snapshot](https://terra-preprod-ui-terra.api.verily.com/workspaces/getting-started-with-workflows-workspace/resources/eebc2df6-fc9d-491e-874c-b87dbd3a68e1/notebook_snapshots/nextflow_examples_second_exercise_fully_executed.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### View available pipelines\n",
    "\n",
    "Dozens of commonly used bioinformatics pipelines are available via [nf-core](https://nf-co.re/pipelines). Now that you have installed `nf-core`'s companion tooling on this instance, run the cell below to  \n",
    "view a list of available Nextflow pipelines in the `nf-core` collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nf-core list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modify nf-core configuration file\n",
    "\n",
    "Next, run the cell below to inject the necessary parameters into the Nextflow configuration file and output its contents.  \n",
    "In the `gls` entry, note the parameters set to workspace context variables: \n",
    "* `$WORKBENCH_{BUCKET_REFERENCE}`, \n",
    "* `$GOOGLE_CLOUD_PROJECT` and \n",
    "* `$GOOGLE_SERVICE_ACCOUNT_EMAIL`. \n",
    "\n",
    "If you've created the workspace bucket resource after you created this notebook instance (via either the [Verily Workbench workspace UI](https://support.workbench.verily.com/docs/guides/research_data/data_resources_operations/#create-a-storage-bucket) or [Workbench CLI commands](https://support.workbench.verily.com/docs/references/cli_reference/wb/resource/create/), your bucket reference may not resolve correctly due to a stale workspace context cache. To resolve the variable, you'll need to run `wb resource list` to refresh the cached workspace context, then re-run the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "def configure_nf_core_for_wb():\n",
    "    \"\"\"\n",
    "    Injects workspace context variables for parameters & configures Nextflow for nf_core RNASeq.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('/home/jupyter/nextflow.config'):\n",
    "        os.mknod('/home/jupyter/nextflow.config')\n",
    "        \n",
    "    wb_nf_core_config = f\"\"\"\n",
    "    profiles {{\n",
    "        gls {{\n",
    "            // Uncomment below line for debugging\n",
    "            //process.echo = true\n",
    "            // Do not change\n",
    "            process.executor = 'google-lifesciences'\n",
    "            process.container = 'nextflow/rnaseq-nf:latest'\n",
    "            process.maxRetries = 5\n",
    "            process.genome = \"R64-1-1\"\n",
    "            process.errorStrategy = {{ task.exitStatus==14 ? 'retry' : 'terminate' }}\n",
    "            workDir = \"${{WORKBENCH_ws_files_autodelete_after_two_weeks}}/nf_core/work\"\n",
    "            google.region  = \"us-central1\"\n",
    "            google.project = \"$GOOGLE_CLOUD_PROJECT\"\n",
    "            google.lifeSciences.network = 'network'\n",
    "            google.lifeSciences.subnetwork = 'subnetwork'\n",
    "            google.lifeSciences.bootDiskSize = '50 GB'\n",
    "            google.lifeSciences.serviceAccountEmail = \"$GOOGLE_SERVICE_ACCOUNT_EMAIL\"\n",
    "            email_on_fail = \"$OWNER_EMAIL\"\n",
    "            storage {{\n",
    "                parallelThreadCount=1\n",
    "        }}\n",
    "    }}\n",
    "  }}\"\"\"\n",
    "    with open(\"/home/jupyter/nextflow.config\", \"w+\") as config_file:\n",
    "        config_file.write(wb_nf_core_config)\n",
    "        print(wb_nf_core_config)\n",
    "        config_file.close()\n",
    "\n",
    "# Copy existing configuration file, if it exists, modifying it.\n",
    "![ -f \"/home/jupyter/nextflow.config\" ] && cp /home/jupyter/nextflow.config /home/jupyter/unmodified_nextflow.config\n",
    "\n",
    "# Inject configuration.\n",
    "configure_nf_core_for_terra()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### View expanded configuration\n",
    "\n",
    "Run the cell below to output the Nextflow configuration's fully-expanded values to the file `expanded_nf_core.config`, then print the parameters and their values (injected from the Verily Workbench workspace context and by the function above) to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wb nextflow -c /home/jupyter/nextflow.config config nf-core/rnaseq -profile gls > expanded_nf_core.config\n",
    "!grep 'workDir' expanded_nf_core.config\n",
    "!grep -A 25 'google {'  expanded_nf_core.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Get inputs for RNASeq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The input for RNASeq is raw FastQ sequencing data. Given sequencing data and a reference genome, the pipeline performs [a series of operations](https://nf-co.re/rnaseq#pipeline-summary), ultimately producing results including alignments, gene counts and a quality-control report. \n",
    "<a id=\"workspace_setup\"> \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> Please run *only one* of the following two cells depending on whether:\n",
    "    <ul>\n",
    "    <li> This notebook exists in a workspace which you cloned from the <a href=\"TODO_LINK_TO_WS\">Getting Started with Nextflow workspace</a></li> OR\n",
    "    <li>You imported this notebook into a new or existing personal workspace which is not a clone of the <a href=\"TODO_LINK_TO_WS\">Getting Started with Nextflow workspace</a>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "**If your workspace is a clone of the [Getting Started with Nextflow workspace](http://todo),** the referenced resource, `test-datasets`, is already present. You'll need to check out the branch containing the RNASeq test data by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd /home/jupyter/test-datasets && git checkout rnaseq\n",
    "!cd /home/jupyter/test-datasets && cat samplesheet/samplesheet_minimal.csv || echo \"Something's not quite right. Please ensure you've added the Git repo as referenced resource and checked out the RNASeq branch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "**If your workspace is NOT a clone of the [Getting Started with Nextflow workspace](http://todo),** you'll need to run the cell below to clone the GitHub repository containing nf-core test data into this cloud environment, then check out the branch containing RNASeq-specific test datasets and sample sheets. Run the following cell to validate that the referenced resource exists and check out the appropriate branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "![[ -f test-datasets/samplesheet/samplesheet_minimal.csv ]] && echo \"Resource already exists\" || (wb resource add-ref git-repo --name=test-datasets --repo-url=git@github.com:nf-core/test-datasets.git && cd /home/jupyter && wb git clone --resource=nf-core-sample-data-repo &&  git checkout rnaseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run the RNASeq pipeline from `nf-core`\n",
    "\n",
    "\n",
    "Run the cell below to launch the nf-core RNASeq pipeline with Google Cloud Life Sciences API as the executor on a dataset of reads from seven [_S. cerevisiae_](https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000146045.2/) samples. Each run should take about one hour to complete. \n",
    "\n",
    "The nf_core RNASeq pipeline produces a large volume of informative output.\n",
    "When inspecting the output, take note each time the executor you configured, the Google Life Sciences API, is invoked to execute the workflow steps that comprise RNASeq, from initial processing to alignment and creation of BAM files to creation of the MultiQC output files.\n",
    "\n",
    "When the pipeline run has completed successfully, the last few lines of output should resemble:\n",
    "```\n",
    "Waiting for file transfers to complete (1 files)\n",
    "-[nf-core/rnaseq] Pipeline completed successfully-\n",
    "Completed at: DD-Month-YYY HH:SS:MM\n",
    "Duration    : 58m 56s\n",
    "CPU hours   : <HOURS>\n",
    "Succeeded   : <NUBMER of TASKS>\n",
    "\n",
    "Restoring the original gcloud project configuration: <GOOGLE-CLOUD-PROJECT-ID>\n",
    "Updated property [core/project].\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "tags": [
     "\"output_scroll\"",
     "\"scroll\":\"true\""
    ]
   },
   "outputs": [],
   "source": [
    "!wb nextflow -c /home/jupyter/nextflow.config run nf-core/rnaseq -profile gls \\\n",
    "-w $(wb resource resolve \\\n",
    "--name=ws_files_autodelete_after_two_weeks)/nf_core/wd -r 3.11.1 \\\n",
    "--outdir $(wb resource resolve --name=ws_files_autodelete_after_two_weeks)/nf_core/output \\\n",
    "--input /home/jupyter/test-datasets/samplesheet/samplesheet_minimal.csv \\\n",
    "--genome 'R64-1-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### View pipeline runs\n",
    "\n",
    "Run the cell below to print a history of the Nextflow pipelines you've executed. The latest entry should be something like:\n",
    "```\n",
    "TIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                                 COMMAND      \n",
    "YYYY-MM-DD HH:MM:SS     1h 3m 34s      <run_name> OK      6e1e448f53      8c1afd3c-d318-4d7d-bb07-139ecf711193       nextflow -c /home/jupyter/nextflow.config run nf-core/rnaseq -profile google -w 'gs://terra-vdevel-genial-olive-3455-ws-files/nf_core/wd' -r 3.10.1 --minAssignedFrags 1 --outdir 'gs://terra-vdevel-genial-olive-3455-autodelete-after-two-weeks/nf_core/output' --input /home/jupyter/terra-solutions-mc-terra-testing/1000_genomes/test-datasets/samplesheet/samplesheet_minimal.csv --genome R64-1-1 --google-debug true --save-reference true     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional: Resume pipeline execution from cached data\n",
    "\n",
    "Nextflow pipelines can be run with the [`--resume` flag](https://www.nextflow.io/docs/latest/getstarted.html#modify-and-resume). This flag causes the pipeline run to rely on existing, cached intermediate results for some tasks; computation of new intermediate results is performed only where inputs or the pipeline have been modified. This feature allows greater efficiency and speed in pipeline development and debugging.\n",
    "\n",
    "To resume pipeline execution for a specific run, provide the session ID of the desired run following the `--resume` flag in the cell below (session ID should be output by the `nextflow log` command in previous cell). If no modifications have been made to the pipeline or its configuration, resuming a successful run should take about 20 minutes. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> \n",
    " The output directory in the command to resume workflow execution (below) differs from the output directory in the original command to launch the workflow. This a necesssary workaround due to a <a href=\"https://github.com/nextflow-io/nextflow/issues/1189https://github.com/nextflow-io/nextflow/issues/1189\">known Nextflow bug</a> in which an error is thrown in instances where an output file of the same name already exists in the output directory (due to the previous run executing successfully and publishing the MultiQC report to the expected output path). If the workflow run you are resuming did not succeed, you may change the output directory to the original destination in the command below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": [
     "\"output_scroll\""
    ]
   },
   "outputs": [],
   "source": [
    "!wb nextflow -c /home/jupyter/nextflow.config run nf-core/rnaseq -resume <SESSION_ID> -profile gls \\\n",
    "-w $(wb resource resolve --name=ws_files_autodelete_after_two_weeks)/nf_core/wd -r 3.11.1 \\\n",
    "--outdir $(wb resource resolve --name=ws_files_autodelete_after_two_weeks)/nf_core/output/resumed \\\n",
    "--input /home/jupyter/test-datasets/samplesheet/samplesheet_minimal.csv \\\n",
    "--genome 'R64-1-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### View execution details for a specific run\n",
    "To view the tasks executed by your pipeline during a specific run, replace the \"RUN_NAME\" parameter below with the run name corresponding to the nf_core RNASeq job from the output of the previous `nextflow log ...` command. Then run the cell below to see the tasks and their statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow log <RUN_NAME> -f 'task_id,name,status,duration,cpus,memory,container' | sort -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### View workflow results\n",
    "\n",
    "The result of a successful run of the example workflow includes an HTML report from [MultiQC](https://multiqc.info/).\n",
    "To view the results, you'll need to obtain this report from the workspace bucket in the output directory provided in a previous step. If you haven't changed the default, you can run the cell below to copy the report to the home directory of this cloud environment.\n",
    "\n",
    "The first time you open the report, it may include a message about JavaScript being disabled.\n",
    "To resolve this, select the \"Trust HTML\" button as described in [this JupyterLab issue](https://github.com/jupyterlab/jupyterlab/issues/9738).\n",
    "Alternatively, you may right click and open the MultiQC report in a new browser tab, which will not have the same issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wb gsutil cp $(wb resource resolve --name=ws_files_autodelete_after_two_weeks)/nf_core/output/multiqc/star_salmon/multiqc_report.html /home/jupyter/nf_core_multiqc_report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Optional: Clean up intermediate results\n",
    "\n",
    "If you won't need to reference the intermediate results produced by each workflow step in the future and don't intend to run a pipeline again with the `--resume` flag, it's a good practice to remove the associated working directories & their contents from your workspace bucket. This reduces your storage cost and makes it easier to find results within your bucket when you do wish to inspect intermediate results from a particular run (i.e. for debugging or to analyze a particular step in the pipeline).   \n",
    "\n",
    "Once you've performed cleanup for a specific run, the associated cached intermediate results have been deleted and you can no longer resume pipeline execution from those results. Therefore, it's important to perform a dry-run of the cleanup first to view the directories that will be removed and ensure you don't inadvertently clean up any checkpointing you wish to keep. \n",
    "\n",
    "Run the cell below to perform a dry-run for a pipeline run by replacing \"RUN_NAME\" with the run name of your specific pipeline execution. If you'd like to cleanup for all previous pipeline runs, replace `<RUN_NAME>` with `$(nextflow log -q)` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow clean -n <RUN_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "If you're satisfied with deleting the pipeline working directories indicated above, replace `<RUN_NAME>` with the name of the desired pipeline execution in the cell below, then run the cell to permanently remove files associated with that run. (If you'd prefer to clean up directories associated with all previous runs, you can replace `<RUN_NAME>` with `$(nextflow log -q)`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow clean -f <RUN_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Provenance\n",
    "\n",
    "Generate information about this notebook environment and the packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "Conda and pip installed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "JupyterLab extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter labextension list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Number of cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep ^processor /proc/cpuinfo | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "Memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"^MemTotal:\" /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright 2022 Verily Life Sciences LLC\n",
    "\n",
    "Use of this source code is governed by a BSD-style   \n",
    "license that can be found in the LICENSE file or at   \n",
    "https://developers.google.com/open-source/licenses/bsd"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/r-cpu:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
