{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Create a Hail cluster on Verily Workbench\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/verily-src/workbench-examples/blob/main/dataproc/create_hail_cluster.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook demonstrates how to create Dataproc (managed Spark) clusters, with [Hail](https://hail.is/) installed, on [Verily workbench](https://support.workbench.verily.com/docs/) **using the Workbench CLI**. (To instead create a cluster using the Workbench web UI, see [this guide](https://support.workbench.verily.com/docs/how_to_guides/dataproc/)).\n",
    "It also discusses how to access and run jobs on the JupyterLab server on the cluster, and how to and access debugging consoles such as the Spark console.\n",
    "\n",
    "This notebook is most straightforward to run from a Workbench workspace [notebook cloud environment](https://support.workbench.verily.com/docs/how_to_guides/using_cloud_environments/), which will pre-install the Workbench CLI.  You can use the default settings when you create the notebook environment. \n",
    "\n",
    "You can also run these commands from other environments, like your local machine, if you [install and configure](https://support.workbench.verily.com/docs/getting_started/install_and_run/) the Workbench CLI. The example assumes that you have already created a Workbench [workspace](https://support.workbench.verily.com/docs/getting_started/web_ui/#creating-a-new-workspace). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "## Objective\n",
    "\n",
    "In this tutorial you will learn how to run [Hail](https://hail.is/) via [Dataproc](https://cloud.google.com/dataproc/docs/concepts/overview) with [autoscaling](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#what_is_autoscaling) for resource management. The steps include:\n",
    "\n",
    "1. Do some initial setup and configuration.\n",
    "1. Create a Dataproc cluster with Hail installed.\n",
    "1. Access JupyterLab and the Spark console running on the cluster.\n",
    "1. Submit a script to the cluster for Hail to run in batch mode.\n",
    "1. (Optional) Pause or delete the cluster.\n",
    "\n",
    "## How to run this notebook\n",
    "\n",
    "Run this notebook cell by cell to create and use a Dataproc cluster with Hail installed.\n",
    "\n",
    "## Costs\n",
    "\n",
    "This notebook takes less than a minute to run, which will typically cost less than $0.01 of compute time on your cloud environment. **This estimate does not include the [cost](https://cloud.google.com/dataproc/pricing) of running the Dataproc cluster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Setup and Configuration\n",
    "\n",
    "## Set some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Obtain the user name so that it can become part of the Hail cluster name. This is useful when people collaborate in Verily Workbench workspaces and want to differentiate their clusters from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = os.getenv('WORKBENCH_USER_EMAIL')\n",
    "if USER:\n",
    "    USER = USER.split('@')[0].replace('.', '-')\n",
    "else:\n",
    "    print('WORKBENCH_USER_EMAIL not defined; using USER')\n",
    "    USER = os.getenv('USER')\n",
    "print(USER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Construct a cluster name from USER and date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HAIL_CLUSTER_NAME = '-'.join(['hail', USER, datetime.now().strftime('%Y%m%d')])\n",
    "\n",
    "print(HAIL_CLUSTER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Confirm your workspace config and create a GitHub repo reference\n",
    "\n",
    "Next, run the following command to check that the CLI is set to use the Workbench *workspace* in which you would like to create the Dataproc cluster. \n",
    "\n",
    "If you are running this notebook _from_ a cloud environment in a Workbench workspace, you should be set.   \n",
    "If you are running the notebook locally and would like to change the workspace, you can do so via `wb workspace set --id=<your-workspace-id>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wb workspace describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Then, create a [*referenced resource*](https://support.workbench.verily.com/docs/how_to_guides/add_repo_to_ws/) for the GitHub repo that holds the Dataproc example notebooks (the repo that this notebook comes from).\n",
    "This allows the example notebooks to be automounted to the file system of the Dataproc cluster's main node, so that once the cluster is created, you can access them easily.\n",
    "\n",
    "Note: If you are running this notebook from a Workbench workspace, this reference may already be defined. It is harmless to run this command more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wb resource resolve --name workbench-examples || wb resource add-ref git-repo --repo-url https://github.com/verily-src/workbench-examples.git \\\n",
    "  --name workbench-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Confirm your `gcloud` settings\n",
    "\n",
    "As part of this example, you will use the [gcloud](https://cloud.google.com/sdk/docs/install) SDK to upload a cluster autoscaling policy.  **If you are running this example from a Workbench cloud environment, `gcloud` will already be installed and properly configured** to use your workspace's underlying Google Cloud project, and you don't need to take any action.\n",
    "\n",
    "However, if you're running this example from a different environment, you may need to first run:\n",
    "\n",
    "```sh\n",
    "gcloud config set project <your-project-id>\n",
    "```\n",
    "\n",
    "You can find your workspace's project ID on the overview page for the workspace in the Workbench web UI, or from the command line via `wb workspace describe`.\n",
    "\n",
    "Alternately, you can pass the `--project=<your-project-id>` argument to gcloud commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Define an autoscaling policy\n",
    "\n",
    "Configure Dataproc [autoscaling](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling) to automatically and dynamically scale the number of worker VMs in Dataproc clusters to meet workload demands.\n",
    "\n",
    "People will likely have many different autoscaling policies, since some jobs will run best with different numbers of primary workers that will not be preempted.\n",
    "\n",
    "Here, we're creating the policy via the command line, but you can also do so [via the Google Cloud Console](https://support.workbench.verily.com/docs/how_to_guides/dataproc/#creating-an-autoscaling-policy-via-the-cloud-console)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile two_worker_autoscaling_policy.yaml\n",
    "\n",
    "workerConfig:\n",
    "  # Best practice: keep min and max values identical for primary workers\n",
    "  # https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#avoid_scaling_primary_workers\n",
    "  minInstances: 2\n",
    "  maxInstances: 2\n",
    "secondaryWorkerConfig:\n",
    "  maxInstances: 50\n",
    "basicAlgorithm:\n",
    "  cooldownPeriod: 4m\n",
    "  yarnConfig:\n",
    "    scaleUpFactor: 0.05\n",
    "    scaleDownFactor: 1.0\n",
    "    gracefulDecommissionTimeout: 1h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Import the autoscaling policy, if it does not yet exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc autoscaling-policies describe two_worker_autoscaling_policy --region=us-central1 || \\\n",
    "    gcloud dataproc autoscaling-policies import two_worker_autoscaling_policy \\\n",
    "        --source=two_worker_autoscaling_policy.yaml \\\n",
    "        --region=us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Create a Dataproc cluster with Hail installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Now, we'll run the command to create a cluster.  For this example, we're only setting a few parameters.  The `--software-framework==HAIL` parameter specifies the Hail installation. The `--idle-delete-ttl` parameter indicates to delete the cluster after it is idle for the given period (in seconds). If this parameter is not included, the cluster will not auto-delete.\n",
    "\n",
    "Run `wb resource create dataproc-cluster` to see more [options](https://support.workbench.verily.com/docs/guides/cloud_environments/dataproc/).   \n",
    "Many of the parameters of this command are passed through from the `gcloud dataproc` command. See the Dataproc [docs](https://cloud.google.com/dataproc/docs/) and [reference documentation](https://cloud.google.com/dataproc/docs/reference/rest/) for more detail on these parameters.\n",
    "\n",
    "\n",
    "(You can also create a cluster via the Workbench web UI. To do so, see [this guide](https://support.workbench.verily.com/docs/how_to_guides/dataproc/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wb resource create dataproc-cluster \\\n",
    "  --name={HAIL_CLUSTER_NAME} \\\n",
    "  --software-framework=HAIL \\\n",
    "  --num-workers 2 \\\n",
    "  --autoscaling-policy two_worker_autoscaling_policy --idle-delete-ttl 3600s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "The cluster will take a few minutes to start up.  If you like, you can view your new cloud environment in the [Workbench web UI](https://workbench.verily.com/workspaces), under the *Resources* tab, as [described here](https://support.workbench.verily.com/docs/how_to_guides/dataproc/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Access the JupterLab server on the cluster as well as the debugging consoles\n",
    "\n",
    "You can use the **JupyterLab URL** printed by the next cell to access JupyterLab running on the cluster. See also the URLs to the debuging consoles such as the Spark Console.\n",
    "\n",
    "You can also find the link to your new cluster's JupyterLab server by visiting the **Resources** tab for your workspace in the Workbench Web UI, as [described here](https://support.workbench.verily.com/docs/how_to_guides/dataproc/).\n",
    "\n",
    "Alternately, you can find the link directly the Cloud Console if you like:\n",
    "* Go to the Cloud Console -> Dataproc -> Clusters\n",
    "* Select the cluster on which you want to run the notebook\n",
    "* Click on tab 'WEB INTERFACES'\n",
    "* Click on 'JupyterLab'\n",
    "\n",
    "CPU utilization, memory utilization, and other performance metrics for the cluster are available on the Cloud Console. Click on the cluster name to see the plots of these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters describe {HAIL_CLUSTER_NAME} --region=us-central1 \\\n",
    "  --format=\"yaml(config.endpointConfig.httpPorts)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Use Hail on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Use Hail interactively on the cluster's JupyterLab server\n",
    "\n",
    "In the output of the section \"Access JupyterLab and the debugging consoles\" above, click the **JupyterLab** link, or use one of the other methods described above to access the cluster's JupyterLab server (e.g., visit the Workbench web UI).   \n",
    "You can also find this link under the **WEB INTERFACES** tab when you click in to the details for your cluster in the [Cloud Console](https://console.cloud.google.com/dataproc).\n",
    "\n",
    "From the JupyterLab server, open the `/home/jupyter/repos/workbench-examples/dataproc/annotate_significant_gwas_results_with_gnomad.ipynb` notebook. \n",
    "This notebook is available on the cluster's JupyterLab server because we added the [workbench-examples repo](https://github.com/verily-src/workbench-examples.git) as a reference to the workspace before creating the cluster— the `wb resource add-ref git-repo ...` command above.\n",
    "\n",
    "In the notebook, you may want to try setting the `INTERVALS_TO_EXAMINE` constant to `['chr1-chr22']`, to run at scale.  This should cause the cluster's *autoscaling* to kick in.\n",
    "\n",
    "As the cluster scales up, you can monitor in the Cloud Console's Dataproc dashboard (e.g. see the YARN nodes panel):\n",
    "\n",
    "<img src=\"https://github.com/verily-src/workbench-examples/raw/main/dataproc/images/dataproc_dashboard.png\" width=\"70%\">\n",
    "\n",
    "When the job(s) are done, you can monitor its turndown:\n",
    "\n",
    "<img src=\"https://github.com/verily-src/workbench-examples/raw/main/dataproc/images/dataproc_dashboard2.png\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Submit a Hail batch job\n",
    "\n",
    "The notebook [batch_job_submit.ipynb](https://github.com/verily-src/workbench-examples/blob/main/dataproc/batch_job_submit.ipynb) (in the same repo directory as this notebook) walks you through submission of a batch job to the cluster. \n",
    "\n",
    "To run that notebook, you will need to know the name of your cluster.  \n",
    "\n",
    "The batch job notebook does not need to be run _on_ the cluster's server— you can run it from a notebook cloud environment in your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to be reminded of your cluster name\n",
    "print(HAIL_CLUSTER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Once the batch job is running (or after it has finished), you can view the [cluster dashboard](https://console.cloud.google.com/dataproc) (click in to view detail for each cluster) and the [job info](https://console.cloud.google.com/dataproc/jobs), including job logs, in the Google Cloud console. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Stop or delete your cluster when you are finished\n",
    "\n",
    "## Stopping a cluster\n",
    "\n",
    "Verily Workbench does not currently support \"autopause\"— so you may want to **stop** (pause) a cluster while you're not using it.  Alternately, your cluster will automatically be deleted after `idle-delete-ttl` seconds of activity.  For the cluster created above, we set this value to 3600s, or one hour.\n",
    "\n",
    "You can stop the cluster via `gcloud` from the command line, or via the Workbench UI. See [this guide](https://support.workbench.verily.com/docs/how_to_guides/dataproc/) for details on how to do so from the web UI. \n",
    "\n",
    "You can run the next cell to stop the cluster from the command line.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If autoscaling has been initiated, it may not be possible to <code>STOP</code> the cluster, only to <code>DELETE</code> it.<br/>   \n",
    "    Even if your cluster has been stopped, it will still delete itself after the <code>idle-delete-ttl</code> period of inactivity.\n",
    "</div>\n",
    "\n",
    "You can also visit the [cluster dashboard](https://console.cloud.google.com/dataproc) in the Cloud Console to delete your cluster.  \n",
    "* Go to the Cloud Console -> Dataproc -> Clusters\n",
    "* Select the cluster on which you want to stop\n",
    "* Click on 'Stop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this command to STOP your cluster\n",
    "# !gcloud dataproc clusters stop {HAIL_CLUSTER_NAME} --region=us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Deleting a cluster\n",
    "\n",
    "To delete a cluster, use the Workbench web UI or CLI (You cannot delete Workbench clusters via the Cloud Console).  See [this guide](https://support.workbench.verily.com/docs/how_to_guides/dataproc/) for details on how to do so from the Workbench web UI.  To delete a cluster using the CLI, uncomment and run the command below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this command to DELETE your cluster\n",
    "# !wb resource delete --name {HAIL_CLUSTER_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Provenance\n",
    "\n",
    "Generate information about this notebook environment and the packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Conda and pip installed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "JupyterLab extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter labextension list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Number of cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep ^processor /proc/cpuinfo | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"^MemTotal:\" /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright 2023 Verily Life Sciences LLC\n",
    "\n",
    "Use of this source code is governed by a BSD-style   \n",
    "license that can be found in the LICENSE file or at   \n",
    "https://developers.google.com/open-source/licenses/bsd"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/r-cpu:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
