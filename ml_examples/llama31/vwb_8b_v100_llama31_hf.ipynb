{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97a56e7-ea1d-4594-8e94-c4852b2f5b46",
   "metadata": {},
   "source": [
    "# Run LLAMA 3.1 in a notebook\n",
    "\n",
    "This example shows how to load and run inference with a [Llama 3.1 model](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) on Verily Workbench, using the [Huggingface](https://huggingface.co/) libraries and model access.\n",
    "\n",
    "You'll need a Huggingface [account](https://huggingface.co/join) and [access token](https://huggingface.co/settings/tokens). You'll also need to apply for *approval to access the Llama 3.1 model files*.  You'll find a link to do that when you access one of the Llama models from Huggingface, e.g.: \n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\n",
    "\n",
    "This notebook uses the Huggingface `transformers` library for model inference, and uses the LLama 3.1 8B-Instruct model: \n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\n",
    "\n",
    "Create a **Verily Workbench JupyterLab notebook environment** to run this example. **Use 8 CPUs, and 1 v100 GPU**.  With that configuration, the notebook costs ~3.01/hr to run.  \\\n",
    "Pick the **TensorFlow image** when you create the notebook environment. (Below, we'll install `torch`. This gives us a newer version of `torch` than that used by the Pytorch notebook environment, for better memory management).\n",
    "\n",
    "Note: the larger Llama 3.1 models need more powerful GPUs and will not run with the above configuration.  See the end of this example for a bit more discussion on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff943376-d1e1-4d35-8fe0-b178c3edf75e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before you get started, make sure you have your Huggingface access token available.\n",
    "\n",
    "First, install some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86b2db-37df-4176-a50e-7c20baf1d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4f530-a9b0-4a9c-910e-bdc4fc7de39d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Restart the kernel before proceeding**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fcbb0-da4e-4609-a251-d7ec127dd067",
   "metadata": {},
   "source": [
    "Do some imports, and set the model ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3d5f1-a82a-4a44-97d1-0d7a9cd0b3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "# The above notebook configuration will not support the 70B model.  See the end of the notebook for more discussion.\n",
    "# model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99f2dd-2841-4324-a59c-c570327fe8e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc288cf5-f96b-49f8-8c7e-f297044e7f27",
   "metadata": {},
   "source": [
    "Load the model. Before running the following cell, edit `YOUR_HF_ACCESS_TOKEN` to **use your access token**.\n",
    "\n",
    "You'll only need to download the model files once; after that, they'll load from the notebook environment's file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194e229-47d8-458e-a8bb-eccfa5535903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    token=\"YOUR_HF_ACCESS_TOKEN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c119810-33be-4d3d-9759-2f61713008dd",
   "metadata": {},
   "source": [
    "## Run inference on the model and view the response\n",
    "\n",
    "We'll formulate the prompt in terms of 'roles'— information for the 'system', and then the 'user' query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d7baa-aa0c-46c3-b2e8-4a809eefe3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about the field of biomedical research.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff5c12-feb0-459e-a7d9-09eb5f17fc03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "chat = response[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e8678-63b5-4457-b81c-9152176793da",
   "metadata": {},
   "source": [
    "For the 8B model on a V100, each inference call will take ~2-5 mins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929fcc3-8620-4db2-8912-bee4d093bbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640bd63-0568-4d07-bd53-9f4a129c34f6",
   "metadata": {},
   "source": [
    "### Append a new query to the existing prompt context\n",
    "\n",
    "We can maintain the existing context as we ask the model additional questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781db32e-c825-4fd8-885a-8117e2e5ef10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat.append( {\"role\": \"user\", \"content\": \"Describe what a GWAS is\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324dfc9-044e-4484-bc53-f1667cafca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = pipeline(\n",
    "    chat,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "chat = response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad7dd3-42d1-450e-b469-2efe72c564a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa296d-bc7f-4fde-ba7b-b9c644393237",
   "metadata": {},
   "source": [
    "\n",
    "Note that because we appended the new query to the previous response context, we're still seeing the response in \"pirate speak\".\n",
    "\n",
    "Next, try refining the query— note that we don't need to provide additional context on what \"more\" means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfd305-71c1-4ad7-97d2-0a85ae368171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat.append( {\"role\": \"user\", \"content\": \"Tell me more.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d9af2-d4b2-400e-b81a-46bea3af0227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = pipeline(\n",
    "    chat,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "chat = response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2331a-099d-40e7-8506-4e820be377f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342419e5-279f-41b3-9f48-6650ca507667",
   "metadata": {},
   "source": [
    "## Augment the prompt with information from a relevant document\n",
    "\n",
    "Download the \"Introduction to Verily Workbench\" document (in Markdown format) as taken from the Workbench [support site](https://support.workbench.verily.com/):\n",
    "\n",
    "**TODO**: update to use main branch path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa891db-83f8-4a24-9cac-d53e33d166ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p documents\n",
    "!wget https://raw.githubusercontent.com/verily-src/workbench-examples/amyu/llama31/ml_examples/llama31/overview.md -O documents/overview.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1577b3-9bde-49cf-b3b9-3ae26d36a6c5",
   "metadata": {},
   "source": [
    "Read the file into a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccc49c-40e0-4b3e-a055-bb267acf92a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/home/jupyter/documents/overview.md'\n",
    " \n",
    "with open(file_path, 'r') as file:\n",
    "    file_content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6002589-0342-4952-a848-b5a97c3c936b",
   "metadata": {},
   "source": [
    "We'll first try a query without using this supplementary information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8486b06-5f17-4df9-9d5a-c3ec1dc78e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Verily Workbench.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa0a40-9547-4bad-b9aa-118919ea811a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601c05f-0add-4e48-aeee-be3de8a8c53e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fc92f-fa5e-4069-9340-d8b4d87f00ae",
   "metadata": {},
   "source": [
    "The above response will **likely not be very accurate** (the larger Llama 3.1 models would typically do a bit better).  \n",
    "\n",
    "We can include a bit more information about Verily Workbench in the prompt to the model. We'll do that by including the Verily Workbench 'Overview' content from the Workbench [support site](https://support.workbench.verily.com/), that we downloaded above. This information will help the model summarize more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198ce28-d723-4ade-bf6c-fe3aeb9eebca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": file_content},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Verily Workbench.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3418298-0bd3-459b-8bce-9ecbc467cede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "chat = response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4efa9-0786-45e2-b1a2-d115b4eb3c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de0a47-244d-44c8-9c97-ea820f613a3d",
   "metadata": {},
   "source": [
    "This response should look more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b45ad-4dd8-48ef-a468-87f1990d8afc",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "GPUs can be expensive; be sure to stop or delete your notebook environment when you are done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9407a5-06b3-460e-8086-e09e072b7bcb",
   "metadata": {},
   "source": [
    "## Experimenting with a larger Llama3.1 model\n",
    "\n",
    "If you want to experiment with using the Llama3.1 70B model, try creating a notebook that uses 2 A100s and has more disk space.  This notebook must be created via the [Verily Workbench CLI](https://support.workbench.verily.com/docs/guides/cli/cli_install_and_run/), as the UI does not support all the necessary config:\n",
    "\n",
    "```\n",
    "wb resource create gcp-notebook --name llama3170b --machine-type=a2-highgpu-2g  --vm-image-family=tf-ent-latest-gpu --vm-image-project=deeplearning-platform-release  --data-disk-size 800 --accelerator-type NVIDIA_TESLA_A100 --accelerator-core-count=2 --install-gpu-driver=true\n",
    "```\n",
    "\n",
    "Most of the examples in this notebook should run with that configuration, with the exception of the last section on \"Augmenting the prompt with information from a relevant document\".  That will likely cause an OoM error.  \n",
    "Each inference call to the 70B model will take ~2 hours with this configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13637363-948a-4af8-bc69-70041c23f322",
   "metadata": {},
   "source": [
    "## Provenance\n",
    "\n",
    "(You can ignore the `huggingface/tokenizers` warnings in the following.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3787f-fd3d-4c8c-87ad-b5bc7056acff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90fb3c-8903-4710-b304-4434c0e1f688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b690f9-32c4-41fd-9eec-96136bfea5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!grep ^processor /proc/cpuinfo | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c2bab-b69e-4a3b-a93e-fefad75c8961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!grep \"^MemTotal:\" /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c9aa6-cd1a-4b3d-a74e-319cead6b885",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Copyright 2024 Verily Life Sciences LLC\n",
    "\n",
    "Use of this source code is governed by a BSD-style \\\n",
    "license that can be found in the LICENSE file or at \\\n",
    "https://developers.google.com/open-source/licenses/bsd"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-16.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-16:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
